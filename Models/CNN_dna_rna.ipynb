{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zepKoJN7W5tQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight, shuffle, resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, cohen_kappa_score, precision_score, recall_score, precision_recall_curve, auc, roc_auc_score, make_scorer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.metrics import Metric\n",
    "#from focal_loss import SparseCategoricalFocalLoss\n",
    "from tensorflow.keras.losses import Loss\n",
    "from keras.losses import BinaryFocalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6ffBQE1W6iy"
   },
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "def load_sample_data(var_path, ref_path, target_path):\n",
    "    var_df = pd.read_csv(var_path, index_col = 0)   # variant reads\n",
    "    ref_df = pd.read_csv(ref_path, index_col = 0)   # reference reads\n",
    "    target_df = pd.read_csv(target_path)            # target labels\n",
    "\n",
    "    # handling missing values\n",
    "    var_df.dropna(axis = 1, how = 'any', inplace = True)\n",
    "    ref_df.dropna(axis = 1, how = 'any', inplace = True)\n",
    "\n",
    "    assert var_df.index.equals(ref_df.index) and var_df.columns.equals(ref_df.columns)\n",
    "\n",
    "    # filtering out the SNVs where MutationType is -1 (neither class)\n",
    "    snvs_to_remove = target_df[target_df['MutationType'] == -1]['SNV']\n",
    "    var_df_filtered = var_df[~var_df.index.isin(snvs_to_remove)]\n",
    "    ref_df_filtered = ref_df[~ref_df.index.isin(snvs_to_remove)]\n",
    "    target_df_filtered = target_df[~target_df['SNV'].isin(snvs_to_remove)]\n",
    "\n",
    "    print('var_filt shape:', var_df_filtered.shape, 'ref_filt shape:', ref_df_filtered.shape, 'target_filt shape:', target_df_filtered.shape)\n",
    "\n",
    "    return var_df_filtered, ref_df_filtered, target_df_filtered['MutationType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xndh2omsW6lD"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(var_df, ref_df, y_arr, explained_variance_threshold=None, apply_svd=False, sample_id=None, output_dir=None):\n",
    "    X_var = var_df.values[:, 1:]  # Ignoring the SNV index\n",
    "    X_ref = ref_df.values[:, 1:]\n",
    "    y_arr = y_arr.astype(int)\n",
    "\n",
    "    # convert classes to binary\n",
    "    y_arr[y_arr == 1] = 0  # DNA class\n",
    "    y_arr[y_arr == 2] = 1  # RNA class\n",
    "\n",
    "    # shuffle data\n",
    "    combined_data = shuffle(np.hstack((X_var, X_ref, y_arr.reshape(-1, 1))))\n",
    "    X_var_shuffled = combined_data[:, :X_var.shape[1]]\n",
    "    X_ref_shuffled = combined_data[:, X_var.shape[1]:X_var.shape[1] + X_ref.shape[1]]\n",
    "    y_shuffled = combined_data[:, -1].astype(int)\n",
    "\n",
    "    # train-val-test split\n",
    "    X_var_temp, X_var_test, X_ref_temp, X_ref_test, y_temp, y_test = train_test_split(X_var_shuffled, X_ref_shuffled, y_shuffled,\n",
    "                                                                                      test_size = 0.2, random_state = 1, stratify = y_shuffled)\n",
    "\n",
    "    X_var_train, X_var_val, X_ref_train, X_ref_val, y_train, y_val = train_test_split(X_var_temp, X_ref_temp, y_temp,\n",
    "                                                                                      test_size = 0.25, random_state = 1, stratify = y_temp)\n",
    "\n",
    "    # Scale data\n",
    "    scaler_var = StandardScaler()\n",
    "    X_var_train_scaled = scaler_var.fit_transform(X_var_train)\n",
    "    X_var_val_scaled = scaler_var.transform(X_var_val)\n",
    "    X_var_test_scaled = scaler_var.transform(X_var_test)\n",
    "\n",
    "    scaler_ref = StandardScaler()\n",
    "    X_ref_train_scaled = scaler_ref.fit_transform(X_ref_train)\n",
    "    X_ref_val_scaled = scaler_ref.transform(X_ref_val)\n",
    "    X_ref_test_scaled = scaler_ref.transform(X_ref_test)\n",
    "\n",
    "\n",
    "    # Scale data using MaxAbsScaler\n",
    "    # scaler_var = MaxAbsScaler()\n",
    "    # X_var_train_scaled = scaler_var.fit_transform(X_var_train)\n",
    "    # X_var_val_scaled = scaler_var.transform(X_var_val)\n",
    "    # X_var_test_scaled = scaler_var.transform(X_var_test)\n",
    "\n",
    "    # scaler_ref = MaxAbsScaler()\n",
    "    # X_ref_train_scaled = scaler_ref.fit_transform(X_ref_train)\n",
    "    # X_ref_val_scaled = scaler_ref.transform(X_ref_val)\n",
    "    # X_ref_test_scaled = scaler_ref.transform(X_ref_test)\n",
    "\n",
    "\n",
    "    # Combine varreads and refreads into 3D tensors (samples, features, channels)\n",
    "    X_train_combined = np.stack((X_var_train_scaled, X_ref_train_scaled), axis = -1)\n",
    "    X_val_combined = np.stack((X_var_val_scaled, X_ref_val_scaled), axis = -1)\n",
    "    X_test_combined = np.stack((X_var_test_scaled, X_ref_test_scaled), axis = -1)\n",
    "\n",
    "    return X_train_combined, X_val_combined, X_test_combined, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0eeZtEjW6nb"
   },
   "outputs": [],
   "source": [
    "def build_1x1_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape = input_shape))\n",
    "\n",
    "    # first 1x1 convolution Block\n",
    "    model.add(layers.Conv1D(filters = 256, kernel_size=1, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # second 1x1 convolution Block\n",
    "    model.add(layers.Conv1D(filters = 256, kernel_size = 1, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # third 1x1 convolution Block\n",
    "    model.add(layers.Conv1D(filters = 256, kernel_size = 1, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # fully connected layers\n",
    "    model.add(layers.Dense(128, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate = 0.001),\n",
    "        loss = focal_loss(alpha = 0.25, gamma = 2.0),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
    "            tf.keras.metrics.Precision(name = 'precision'),\n",
    "            tf.keras.metrics.Recall(name = 'recall'),\n",
    "            tf.keras.metrics.AUC(name = 'auc'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXfSLa7mb2NA"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, sample_id, output_dir):\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # evaluation metrics\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average = 'macro')\n",
    "    precision_macro = precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average = 'macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize = (7, 5))\n",
    "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Blues',\n",
    "                xticklabels = ['DNA', 'RNA'], yticklabels = ['DNA', 'RNA'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {sample_id}')\n",
    "    image_path = os.path.join(output_dir, f'{sample_id}_conf_mat.png')\n",
    "    plt.savefig(image_path, dpi = 300)\n",
    "    plt.close()\n",
    "\n",
    "    return f1_macro, kappa, precision_macro, recall_macro, roc_auc, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ8iflJhecwY"
   },
   "outputs": [],
   "source": [
    "def focal_loss(alpha = 0.25, gamma = 2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # clipping predictions to avoid log(0) error\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        fl = -alpha * (1 - pt) ** gamma * K.log(pt)\n",
    "        return K.mean(fl, axis=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57EQ5cSsW6p_"
   },
   "outputs": [],
   "source": [
    "output_dir_base = \"cnn_binary_results_focal/cnn_fixed_architecture_focal\"\n",
    "\n",
    "sample_ids = ['nanopore_SRR21492154', 'nanopore_SRR21492155', 'nanopore_SRR21492156', 'nanopore_SRR21492158', 'nanopore_SRR21492159']\n",
    "\n",
    "explained_variance_thresholds = [0.95]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    var_path = f\"{sample_id}_varreads.csv\"\n",
    "    ref_path = f\"{sample_id}_refreads.csv\"\n",
    "    target_path = f\"{sample_id}_targets.csv\"\n",
    "    print(f\"\\nProcessing sample: {sample_id}\")\n",
    "\n",
    "    # load data\n",
    "    var_df, ref_df, y_arr = load_sample_data(var_path, ref_path, target_path)\n",
    "\n",
    "    for explained_variance_threshold in explained_variance_thresholds + [None]:\n",
    "        if explained_variance_threshold is None:\n",
    "            output_dir = os.path.join(output_dir_base, \"no_svd\")\n",
    "        else:\n",
    "            output_dir = os.path.join(output_dir_base, f\"svd_{int(explained_variance_threshold * 100)}_variance\")\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    # preprocessing call\n",
    "    X_train_combined_final, X_val_combined_final, X_test_combined_final, y_train, y_val, y_test = preprocess_data(var_df, ref_df, y_arr, apply_svd = False)\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    print('input shape:', X_train_combined_final.shape[1:])\n",
    "\n",
    "\n",
    "    # model training and evaluation calls\n",
    "    model = build_1x1_cnn_model(X_train_combined_final.shape[1:])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor = 'val_loss', min_delta = 0.001, patience = 10, restore_best_weights = True),\n",
    "                 TensorBoard(log_dir = os.path.join(output_dir_base, f\"logs/{sample_id}\"))]\n",
    "\n",
    "    model.fit(X_train_combined_final, y_train,\n",
    "              validation_data = (X_val_combined_final, y_val),\n",
    "              epochs = 50, callbacks = callbacks, verbose = 2)\n",
    "\n",
    "    f1_macro, kappa, precision_macro, recall_macro, roc_auc, conf_matrix = evaluate_model(model, X_test_combined_final,\n",
    "                                                                                          y_test, sample_id, output_dir)\n",
    "\n",
    "    all_results.append({\n",
    "        'sample_id': sample_id,\n",
    "        'f1_macro': f1_macro,\n",
    "        'kappa': kappa,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'roc_auc': roc_auc,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_csv_path = os.path.join(output_dir_base, 'cnn_1x1_results.csv')\n",
    "results_df.to_csv(results_csv_path, index = False)\n",
    "print(\"All results saved to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
