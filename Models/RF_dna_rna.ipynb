{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT1MZIGo-Lsc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, auc, confusion_matrix, cohen_kappa_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZF6jHl--aC0"
   },
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "def load_sample_data(var_path, ref_path, target_path):\n",
    "    var_df = pd.read_csv(var_path, index_col = 0)  # variant reads\n",
    "    ref_df = pd.read_csv(ref_path, index_col = 0)  # reference reads\n",
    "    target_df = pd.read_csv(target_path)           # target labels\n",
    "\n",
    "    # remove missing value columns altogether\n",
    "    var_df.dropna(axis = 1, how = 'any', inplace = True)\n",
    "    ref_df.dropna(axis = 1, how = 'any', inplace = True)\n",
    "\n",
    "    assert var_df.index.equals(ref_df.index) and var_df.columns.equals(ref_df.columns)\n",
    "\n",
    "    # filtering out the SNVs where MutationType is -1 (neither class)\n",
    "    snvs_to_remove = target_df[target_df['MutationType'] == -1]['SNV']\n",
    "    var_df_filtered = var_df[~var_df.index.isin(snvs_to_remove)]\n",
    "    ref_df_filtered = ref_df[~ref_df.index.isin(snvs_to_remove)]\n",
    "    target_df_filtered = target_df[~target_df['SNV'].isin(snvs_to_remove)]\n",
    "\n",
    "    print('var_filt shape:', var_df_filtered.shape, 'ref_filt shape:', ref_df_filtered.shape, 'target_filt shape:', target_df_filtered.shape)\n",
    "\n",
    "    return var_df_filtered, ref_df_filtered, target_df_filtered['MutationType'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2sO29I0-aFV"
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "def preprocess_data(var_df, ref_df, y_arr, explained_variance_threshold = 0.70, apply_svd = True, sample_id = None, output_dir = None):\n",
    "    X_var = var_df.values[:, 1:]  # ignoring the SNV index\n",
    "    X_ref = ref_df.values[:, 1:]\n",
    "    y_arr = y_arr.astype(int)\n",
    "\n",
    "    # for binary classification\n",
    "    y_arr[y_arr == 1] = 0  # DNA class\n",
    "    y_arr[y_arr == 2] = 1  # RNA class\n",
    "\n",
    "    # combining variant and reference read data\n",
    "    combined_data = np.hstack((X_var, X_ref, y_arr.reshape(-1, 1)))\n",
    "    combined_data_shuffled = shuffle(combined_data)\n",
    "\n",
    "    # re-splitting data into respective sets\n",
    "    X_var_shuffled = combined_data_shuffled[:, :X_var.shape[1]]\n",
    "    X_ref_shuffled = combined_data_shuffled[:, X_var.shape[1] : X_var.shape[1] + X_ref.shape[1]]\n",
    "    y_shuffled = combined_data_shuffled[:, -1].astype(int)\n",
    "\n",
    "    # 60-20-20 train-val-test split\n",
    "    X_var_temp, X_var_test, X_ref_temp, X_ref_test, y_temp, y_test = train_test_split(X_var_shuffled, X_ref_shuffled,\n",
    "                                                                                      y_shuffled, test_size = 0.2, random_state = 1, stratify = y_shuffled)\n",
    "\n",
    "    X_var_train, X_var_val, X_ref_train, X_ref_val, y_train, y_val = train_test_split(X_var_temp, X_ref_temp,\n",
    "                                                                                      y_temp, test_size = 0.25, random_state = 1, stratify = y_temp)\n",
    "\n",
    "    # class distribution in the sample\n",
    "    print(\"Class distribution in this sample:\")\n",
    "    unique, counts = np.unique(y_train, return_counts = True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "\n",
    "    # scaling the data\n",
    "    scaler_var = StandardScaler()\n",
    "    X_var_train_scaled = scaler_var.fit_transform(X_var_train)\n",
    "    X_var_val_scaled = scaler_var.transform(X_var_val)\n",
    "    X_var_test_scaled = scaler_var.transform(X_var_test)\n",
    "\n",
    "    scaler_ref = StandardScaler()\n",
    "    X_ref_train_scaled = scaler_ref.fit_transform(X_ref_train)\n",
    "    X_ref_val_scaled = scaler_ref.transform(X_ref_val)\n",
    "    X_ref_test_scaled = scaler_ref.transform(X_ref_test)\n",
    "\n",
    "    # combining scaled data\n",
    "    X_train_combined = np.hstack((X_var_train_scaled, X_ref_train_scaled))\n",
    "    X_val_combined = np.hstack((X_var_val_scaled, X_ref_val_scaled))\n",
    "    X_test_combined = np.hstack((X_var_test_scaled, X_ref_test_scaled))\n",
    "\n",
    "    if apply_svd:\n",
    "        # dimensionality reduction using TruncatedSVD\n",
    "        initial_svd = TruncatedSVD(n_components = min(X_train_combined.shape[1], 1000))  # generally, ~ 95% variance is explained with < 1000 components\n",
    "        X_train_svd = initial_svd.fit_transform(X_train_combined)\n",
    "\n",
    "        cumulative_variance = np.cumsum(initial_svd.explained_variance_ratio_)\n",
    "        n_comp = np.argmax(cumulative_variance >= explained_variance_threshold) + 1    # no. of components explaining the threshold variance\n",
    "\n",
    "        svd_optimal = TruncatedSVD(n_components=n_comp)\n",
    "        X_train_svd = svd_optimal.fit_transform(X_train_combined)\n",
    "        X_val_svd = svd_optimal.transform(X_val_combined)\n",
    "        X_test_svd = svd_optimal.transform(X_test_combined)\n",
    "\n",
    "        if output_dir and sample_id:\n",
    "            plt.figure(figsize = (7, 5))\n",
    "            plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker = 'o', linestyle = '--')\n",
    "            plt.xlabel('Number of components')\n",
    "            plt.ylabel('Cumulative explained variance')\n",
    "            plt.title(f'TruncatedSVD explained variance - {sample_id}')\n",
    "            plt.axvline(n_comp, color = 'r', linestyle = '--',\n",
    "                        label = f'{round(explained_variance_threshold * 100)}% variance ({n_comp} components)')\n",
    "            plt.legend()\n",
    "            plt_path = os.path.join(output_dir, f'{sample_id}_svd_explained_variance.png')\n",
    "            plt.savefig(plt_path, dpi = 500)\n",
    "            plt.close()\n",
    "\n",
    "        X_train_combined_final = X_train_svd\n",
    "        X_val_combined_final = X_val_svd\n",
    "        X_test_combined_final = X_test_svd\n",
    "    else:\n",
    "        # If not applying SVD, just pass the scaled combined data forward\n",
    "        X_train_combined_final = X_train_combined\n",
    "        X_val_combined_final = X_val_combined\n",
    "        X_test_combined_final = X_test_combined\n",
    "        n_comp = None\n",
    "\n",
    "    return X_train_combined_final, X_val_combined_final, X_test_combined_final, y_train, y_val, y_test, n_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ9i86gk-aHa"
   },
   "outputs": [],
   "source": [
    "# custom Cohen's Kappa scorer\n",
    "def custom_cohen_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "cohen_kappa_scorer = make_scorer(custom_cohen_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4xGau0L-aJt"
   },
   "outputs": [],
   "source": [
    "# MODEL TRAINING AND EVALUATION\n",
    "def train_and_evaluate_models(X_train, X_val, X_test, y_train, y_val, y_test, sample_id, output_dir):\n",
    "    X_train_comb = np.vstack((X_train, X_val))\n",
    "    y_train_comb = np.concatenate((y_train, y_val))\n",
    "\n",
    "    # PredefinedSplit for setting custom validation set\n",
    "    test_fold = [-1] * len(X_train) + [0] * len(X_val)\n",
    "    ps = PredefinedSplit(test_fold = test_fold)\n",
    "\n",
    "    # computing class weights\n",
    "    # classes = np.unique(y_train_comb)\n",
    "    # class_weights = compute_class_weight(class_weight = 'balanced', classes = classes, y = y_train_comb)\n",
    "    # class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = 200,\n",
    "        criterion = 'gini',\n",
    "        max_depth = None,\n",
    "        min_samples_split = 4,\n",
    "        min_samples_leaf = 2,\n",
    "        bootstrap = True,\n",
    "        class_weight = 'balanced'\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train_comb, y_train_comb)\n",
    "\n",
    "    # evaluation\n",
    "    y_pred = rf.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize = (7, 5))\n",
    "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Blues',\n",
    "                xticklabels = ['DNA', 'RNA'], yticklabels = ['DNA', 'RNA'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for Random Forest - {sample_id}')\n",
    "    conf_matrix_path = os.path.join(output_dir, f'{sample_id}_RandomForest_conf_mat.png')\n",
    "    plt.savefig(conf_matrix_path, dpi = 300, bbox_inches = 'tight', pad_inches = 0)\n",
    "    plt.close()\n",
    "\n",
    "    # model parameters\n",
    "    hyperparams_path = os.path.join(output_dir, f'{sample_id}_RandomForest_fixed_hyperparameters.txt')\n",
    "    with open(hyperparams_path, 'w') as f:\n",
    "        for param, value in rf.get_params().items():\n",
    "            f.write(f'{param}: {value}\\n')\n",
    "\n",
    "    return rf, \"Random Forest\", X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHhmMNu0-aMT"
   },
   "outputs": [],
   "source": [
    "def run_best_model_multiple_times(fixed_model_class, model_name, X_train, y_train, X_test_scaled, y_test, output_dir, sample_id, n_runs = 1):\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        model = fixed_model_class(\n",
    "            n_estimators = 200,\n",
    "            criterion = 'gini',\n",
    "            max_depth = None,\n",
    "            min_samples_split = 4,\n",
    "            min_samples_leaf = 2,\n",
    "            bootstrap = True,\n",
    "            class_weight = 'balanced',\n",
    "            random_state = i\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_probs = model.predict_proba(X_test_scaled)\n",
    "        y_pred = np.argmax(y_pred_probs, axis = 1)\n",
    "\n",
    "        f1_macro = f1_score(y_test, y_pred, average = 'macro')\n",
    "        precision_macro = precision_score(y_test, y_pred, average = 'macro')\n",
    "        recall_macro = recall_score(y_test, y_pred, average = 'macro')\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_probs[:, 1])\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'sample_id': sample_id,\n",
    "            'run': i + 1,\n",
    "            'model': model_name,\n",
    "            'f1_macro': f1_macro,\n",
    "            'kappa': kappa,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'roc_auc': roc_auc\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv_path = os.path.join(output_dir, f'{sample_id}_{model_name}_{n_runs}runs_metrics.csv')\n",
    "    results_df.to_csv(results_csv_path, index = False)\n",
    "\n",
    "    print(f\"All results for {n_runs} runs of {model_name} saved to CSV.\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QfIJ8QH-aON",
    "outputId": "1f07d793-94f0-48ef-8e38-5715a3cb176a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sample: nanopore_SRR21492154\n",
      "var_filt shape: (5117, 1192) ref_filt shape: (5117, 1192) target_filt shape: (5117, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 2474, 1: 595}\n",
      "Best model: Random Forest for sample nanopore_SRR21492154\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492154\n",
      "\n",
      "Processing sample: nanopore_SRR21492155\n",
      "var_filt shape: (428, 9366) ref_filt shape: (428, 9366) target_filt shape: (428, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 193, 1: 63}\n",
      "Best model: Random Forest for sample nanopore_SRR21492155\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492155\n",
      "\n",
      "Processing sample: nanopore_SRR21492156\n",
      "var_filt shape: (2888, 3836) ref_filt shape: (2888, 3836) target_filt shape: (2888, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 1397, 1: 335}\n",
      "Best model: Random Forest for sample nanopore_SRR21492156\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492156\n",
      "\n",
      "Processing sample: nanopore_SRR21492157\n",
      "var_filt shape: (5434, 4138) ref_filt shape: (5434, 4138) target_filt shape: (5434, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 2616, 1: 644}\n",
      "Best model: Random Forest for sample nanopore_SRR21492157\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492157\n",
      "\n",
      "Processing sample: nanopore_SRR21492158\n",
      "var_filt shape: (1495, 5482) ref_filt shape: (1495, 5482) target_filt shape: (1495, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 743, 1: 154}\n",
      "Best model: Random Forest for sample nanopore_SRR21492158\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492158\n",
      "\n",
      "Processing sample: nanopore_SRR21492159\n",
      "var_filt shape: (1820, 5916) ref_filt shape: (1820, 5916) target_filt shape: (1820, 2)\n",
      "\n",
      "Running pipeline without TruncatedSVD\n",
      "Class distribution in this sample:\n",
      "{0: 1013, 1: 79}\n",
      "Best model: Random Forest for sample nanopore_SRR21492159\n",
      "All results for 1 runs of Random Forest saved to CSV.\n",
      "Best model: Random Forest for sample nanopore_SRR21492159\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION\n",
    "base_dir = \"\"\n",
    "output_dir_base = os.path.join(base_dir, \"rf_fixed_results\")\n",
    "\n",
    "if not os.path.exists(output_dir_base):\n",
    "    os.makedirs(output_dir_base)\n",
    "\n",
    "sample_ids = ['nanopore_SRR21492154', 'nanopore_SRR21492155', 'nanopore_SRR21492156',\n",
    "              'nanopore_SRR21492157', 'nanopore_SRR21492158', 'nanopore_SRR21492159']\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    var_path = os.path.join(base_dir, f\"{sample_id}_varreads.csv\")\n",
    "    ref_path = os.path.join(base_dir, f\"{sample_id}_refreads.csv\")\n",
    "    target_path = os.path.join(base_dir, f\"{sample_id}_targets.csv\")\n",
    "\n",
    "    print(f\"\\nProcessing sample: {sample_id}\")\n",
    "\n",
    "    var_df, ref_df, y_arr = load_sample_data(var_path, ref_path, target_path)\n",
    "\n",
    "    # preprocess data without SVD\n",
    "    output_dir = os.path.join(output_dir_base, sample_id)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"\\nRunning pipeline without TruncatedSVD\")\n",
    "\n",
    "    X_train_combined_final, X_val_combined_final, X_test_combined_final, y_train, y_val, y_test, n_comp = preprocess_data(var_df, ref_df, y_arr,\n",
    "                                                                                                                          explained_variance_threshold = 0.90, apply_svd = False,\n",
    "                                                                                                                          sample_id = sample_id, output_dir = output_dir)\n",
    "\n",
    "    best_model, best_model_name, X_train_scaled, y_train, X_test_scaled, y_test = train_and_evaluate_models(\n",
    "        X_train_combined_final, X_val_combined_final, X_test_combined_final, y_train, y_val, y_test, sample_id, output_dir\n",
    "    )\n",
    "\n",
    "    print(f\"Best model: {best_model_name} for sample {sample_id}\")\n",
    "\n",
    "    if best_model_name == 'Random Forest':\n",
    "        run_best_model_multiple_times(RandomForestClassifier, best_model_name,\n",
    "                                      X_train_scaled, y_train, X_test_scaled, y_test, output_dir, sample_id, n_runs = 1)\n",
    "\n",
    "    print(f\"Best model: {best_model_name} for sample {sample_id}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
